% Homework 2
% CS 533
% Intelligent Agents and Decision Making
clear; close all;
% Parse the text file:
fileID = fopen('MDPtest.txt','r');

%Intro = textscan(fileID,'%s',4,'Delimiter','\n')
%disp(Intro{1})

StatesActions = textscan(fileID,'%s',2,'Delimiter',' ');
StatesActions = str2double((StatesActions{1}));
num_states = StatesActions(1);
num_actions = StatesActions(2);

% Loop over all actions and create 3D array for actions
% For example, A(:,:,1) will be the dimension for the first action
% Note that A is a transition probability for the states
for a=1:num_actions
    T = textscan(fileID,'%f',num_states*num_states,'Delimiter','\t');
    T = cell2mat(T);
    T = reshape(T,num_states,num_states)';
    A(:,:,a)=T;
end

R = textscan(fileID,'%f',num_states*num_actions,'Delimiter','\t');
R = cell2mat(R);
R = reshape(R,num_actions,num_states)';


% Selecting one reward for both actions since they are the same
Reward = R(:,1);

% Making a 3D array (state, next_state, action(either1or2)
A = A1;
A(:,:,2) = A2;

% Define time steps
tStep = 20;

%MDP (numstates, numactions, actions, rewards)
mdp = MDP(3, 2, A, Reward, tStep);

% Run value iteration
finite_horizon_values = mdp.fh_value_iteration();

% Find the policy
finite_horizon_policy = mdp.fh_policy();